# Import the evaluator
from src.utils.evaluation import ModelEvaluator

# Initialize evaluator
evaluator = ModelEvaluator()

# After training your models
models = {
    'CNN': trained_cnn_model,
    'UNet': trained_unet_model,
    'Transformer': trained_transformer_model,
    'Hybrid': trained_hybrid_model
}

# Evaluate all models
results = {}
for name, model in models.items():
    print(f"\nEvaluating {name} model...")
    metrics = evaluator.evaluate_model(model, test_loader, criterion)
    results[name] = metrics
    
    print(f"\n{name} Results:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

# Visualize comparisons
evaluator.plot_metrics_comparison(results, save_path='results/model_comparison.png')

# Show sample results
evaluator.visualize_results(
    model=models['CNN'],  # or any other model
    dataloader=test_loader,
    num_samples=5,
    save_path='results/cnn_samples.png'
)